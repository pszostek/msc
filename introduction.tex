\chapter{Introduction}
The amount of publications stored in digital libraries is enormous and constantly growing. A modern, fully functional framework for digital libraries, in order to be useful and to provide high quality services, has to have access not only to the full texts of the articles and books that it stores, but also to their metadata. The metadata, data describing data, include such information as: document author(s), document title, document abstract, keywords, document references and others. The purpose of obtaining the metadata is twofold. Firstly, they are indispensible when implementing a search engine, being an invaluable tool for all researchers looking for articles needed in their research field. Secondly, a subset of metadata, i.e. article authors and references are needed for various ranking algorithms based on citation count, such H-index \cite{Hirsch2005}, Impact Factor and other bibliometric indicators.
\section{Problem statement}
Unfortunatelly, usually a digital library has to deal with the resources without any metadata provided by the publisher. In this case, the number of articles makes it barely possible to process the manualy. It might also happen that the metadata available are not trustworthy or are faulty or partially missing. In these cases a digital library needs a method to obtain the metadata automatically or semi-automatically. In the following thesis a system for extracting metadata and references is presented. This system was implemented by the author together with other engineers, although their work will not be described extensively in this document and the authorship of each piece of the system will be marked explicitly. 
\section{Related work}
Extracting metadata from scholarly publications is a well-studied problem. In the past, the algorithms expected scanned documents as an input and were generally formulated as image recoginition problem. Those were built in the period when scholarly communication was present mainly or uniquely in a printed form and each article, before becaming available in a digital way, had to be scanned.
Nowadays, a digital library has to cope with born-digital documents, where the letter recognition stage is ommited and processing starts with building up words and lines of text based on single characters.


Giuffrida et al. [13] describe a metadata extraction system, which processes PostScript files using a tool
based on pstotext, and metadata is extracted by a set of rules and features computed for extracted text chunks. This approach migh be easy and quick to implement, but it's very likely to become inefficient when applying it to a new layout.

Rigamonti et al. [21] present a reverse engineering tool processing PDF documents in order to extract the physical layout structure along with the logical structures. The system has been evaluated against a set of representative newspapers front pages, but was not applied to 
Metadata extraction process presented by Esposito et al. [10]
is able to process both PS and PDF formats. In this ap-
proach page segmentation is done by a kernel-based method
and zones are classified based on machine-learning approach.
Marinai [17] uses JPedal package to extract characters from
PDF documents, page segmentation is done using rule-based
approach, and finally a neural classifier is used for zone clas-