\chapter{Introduction}
Digital libraries gained a lot of momentum in the recent years. The amount of publications stored there is enormous and constantly growing. Moreover, with time their purpose has been evolving. From a place where publications were stored, over search engines to quickly filter out publications relevant to certain topic, to frameworks used for analysis of collaboration patterns, calculation of productivty and efficiency factors such as H-index or G-index or looking for emerging trends and topics in science.
All this research depends on a reliable source of full texts and metadata from analyzed articles.

Even though there was a huge progress in science as such, scholarly publishing relies on methods invented more than two decades ago, which is an immense time in digital world. The most common format in digital publishing is PDF (Portable File Format) which focuses on how the document is rendered in reader's browser, making it highly portable accross hardware and software platforms, but also making it highly unsuitable for carrying metadata, understood here literally as \textit{data about data}.

A modern, fully functional framework for digital libraries, in order to be useful and to provide high quality services, has to have access not only to the full texts of the articles and books that it stores, but also to their metadata. The metadata, data describing data, include such information as: document author(s), document title, document abstract, keywords, document references and others. The purpose of obtaining the metadata is twofold. Firstly, they are indispensible when implementing a search engine, being an invaluable tool for all researchers looking for articles needed in their research field. Secondly, a subset of metadata, i.e. article authors and references are needed for various ranking algorithms based on citation count, such H-index \cite{Hirsch2005}, Impact Factor and other bibliometric indicators.
\section{Problem statement}
Unfortunatelly, usually a digital library has to deal with the resources without any metadata provided by the publisher. In this case, the number of articles makes it barely possible to process them manualy. It might also happen that the metadata available are not trustworthy or are faulty or partially missing. In these cases a digital library needs a method to obtain the metadata automatically or semi-automatically. The following thesis presents CERMINE - a system for extracting metadata and bibliographics references. This system was implemented jointly by the author and his team mates. However, their work will not be described extensively in this document and the authorship of each piece of the system will be marked explicitly. 
\section{Executive summary}
This works presents CERMINE - a system for automatic extraction of metadata from scholarly publications, and GROTOAP2 - Ground-truth publication dataset. The system was developped by the author of this work as a part of his duties in Interdisciplinary Center for Mathematical and Computational Modelling at the University of Warsaw. The system has been recently presented at the Force2015 meeting (\cite{Force2015}). Futhermore, an article on CERMINE was presented in the 11th IAPR International Workshop on Document Analysis Systems (\cite{DominikaTkaczykPaweSzostekMateuszFedoryszakPiotrJanDendek2014}). GROTOAP2 article was published in D-Lib magazine, volume 20. (\cite{DominikaTkaczykPaweSzostek2014}).
Information on the access to the source code and other resources is included in the appendix \ref{appendix:resources}.
\section{Related work}
Extracting metadata from scholarly publications is a well-studied problem. In the past, the algorithms expected scanned documents as an input and were generally formulated as image recoginition problem. Those were built in the period when scholarly communication was present mainly or uniquely in a printed form and each article, before becaming available in a digital way, had to be scanned.
Nowadays, a digital library has to cope with born-digital documents, where the letter recognition stage is omitted and processing starts with building up words and lines of text based on single characters.


% Giuffrida et al. [13] describe a metadata extraction system, which processes PostScript files using a tool
% based on pstotext, and metadata is extracted by a set of rules and features computed for extracted text chunks. This approach migh be easy and quick to implement, but it's very likely to become inefficient when applying it to a new layout.

% Rigamonti et al. [21] present a reverse engineering tool processing PDF documents in order to extract the physical layout structure along with the logical structures. The system has been evaluated against a set of representative newspapers front pages, but was not applied to 
% Metadata extraction process presented by Esposito et al. [10]
% is able to process both PS and PDF formats. In this ap-
% proach page segmentation is done by a kernel-based method
% and zones are classified based on machine-learning approach.
% Marinai [17] uses JPedal package to extract characters from
% PDF documents, page segmentation is done using rule-based
% approach, and finally a neural classifier is used for zone clas-