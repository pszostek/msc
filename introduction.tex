\chapter{Introduction}
Digital libraries gained a lot of momentum in the recent years. The amount of publications held there is enormous and constantly growing. Collecting them in digital repositories raised many problems, whereas many of them go beyond acquisition issues - they have to be organized and classified in order to improve various retrieval procedures. Moreover, with time the purpose of the digital libraries has been evolving from a place where publications were stored, over search engines used to quickly filter out publications relevant for certain topic, to frameworks used for analysis of collaboration patterns, researchers' professional network analysis, calculation of productivity and efficiency factors such as H-index (\cite{Hirsch}) or G-index or looking for emerging trends and topics in science.
All this research depends on a reliable source of full article texts and corresponding metadata, being both an input for analysis algorithms.

Even though there was a huge progress in science as such, scholarly publishing relies on methods invented more than two decades ago, which is an immense time in digital world. The most common format in digital publishing is PDF\footnote{Portable File Format}, which focuses on how the document is rendered in reader's browser making it highly portable across hardware and software platforms, but without putting too much focus on metadata, which are understood here literally as \textit{data about data}. Articles are very often published and distributed in this format resulting in losing metadata. Their recovery can be performed either manually by a human worker or by a computer program. 

Thus, a modern, fully functional framework for digital libraries, in order to be useful and to provide high quality services, has to have access not only to the full texts of the articles and books that it stores, but also to their metadata. The metadata, data describing data, include such information as: document authors, document title, document abstract, keywords, document references and others. The purpose of obtaining the metadata is twofold. Firstly, they are indispensable when implementing a search engine, being an invaluable tool for all researchers looking for articles needed in their research field. Secondly, a subset of metadata, i.e. article authors and references are needed for various ranking algorithms based on citation count, such H-index \cite{Hirsch}, Impact Factor and other bibliometric indicators.

\section{Problem statement}
Regrettably, usually a digital library has to deal with the resources without any metadata provided by the publisher. Even when we consider specialized libraries, whose collections are usually modest, the number of articles makes it barely possible to process them manually. What is more, it might also happen that the metadata available are not trustworthy, are faulty or partially missing. In these cases a digital library needs a way to obtain them automatically or semi-automatically.

This thesis presents CERMINE - a system for automatic extraction of metadata from scholarly publications, as well as GROTOAP2 - Ground-truth publication dataset. At its core CERMINE uses supervised machine learning algorithms, where labeled training samples from GROTOAP2 are used to train the system to recognize and assign individual elements of an article into classes depending on their function in an article. In a multi-stage process CERMINE extracts metadata, full text and bibliographic references from an input article. The system was developed jointly by the author and his team mates as a part of work duties in Interdisciplinary Center for Mathematical and Computational Modeling at the University of Warsaw. However, their work will not be described extensively in this document and the authorship of each piece of the system will be marked explicitly.

\section{Publications}
The system has been recently presented at the Force2015 meeting (\cite{Force2015}). Furthermore, an article on CERMINE was presented in the 11th IAPR International Workshop on Document Analysis Systems (\cite{DominikaTkaczykPaweSzostekMateuszFedoryszakPiotrJanDendek2014}). GROTOAP2 article was published in D-Lib magazine, volume 20. (\cite{DominikaTkaczykPaweSzostek2014}).
Information on the access to the source code and other resources is included in the appendix \ref{app:resources}.

\section{Related work}\label{sec:related_work}
Extracting metadata from scholarly publications is a well-studied problem. In the past, the algorithms expected scanned documents as an input and were generally formulated as image recognition problem. Those were built in the period when scholarly communication was present mainly or entirely in a printed form and each article had to be scanned before becoming available in a digital way. Such approaches were presented in \cite{Thoma2001} and \cite{Flynn2007}.

Nowadays, a digital library has to cope with born-digital documents, where the letter recognition stage is omitted and processing starts with building up words and lines of text based on single characters. In the past several approaches using machine learning methods were presented.

In \cite{Esposito2008} there is presented a method for processing PostScript and PDF files using various machine learning methods in order to extract metadata.

\cite{Marinai2009} extracts basic metadata from scientific papers by, first of all, using various rules to perform segmentation of characters extracted from text and then employing a neural network to assign classes to blocks of text. Results are corrected by comparing them with an external bibliographic database.

In the approach proposed by \cite{Chen.2010} PDF files are first transformed into HTML format, then metadata is extracted using Hidden Markov Model. The authors test their approach using a set of 458 articles for VLDB conferences.

Both \cite{HuiHan} and \cite{HuiHan2005} present an approach of employing mixed Hidden Markov Model/Support Vector Machines algorithm to perform metadata and references extraction. Unlike in our approach, they do not take sequence-oriented information into consideration.

Finally, the TeamBeam algorithm, portrayed in \cite{Kern2012}, uses a Maximum Entropy classifier to include sequence-related information in the classification. This algorithm is applied only to the first page of an article, therefore focusing exclusively on metadata.

\section{Outline of the thesis}
The thesis is divided into six chapters. After this introduction and problem formulation, chapter \ref{chapter:theory} containes theoretical introduction to the statistical learning, statistical classification, as well as to various kinds of Support Vector Machines: linearly separable, non-linearly separable, non-linear and multi-label (as opposed to binary SVMs).
In chapter \ref{chapter:system_workflow} we explain the system architecture and the implemented workflow. 
In chapter \ref{chapter:classification_workflow} we study..
Chapter \ref{chapter:} concentrates on the construction of the training dataset
In chapter \ref{chapter:evaluation}
Also, this thesis contains 5 appendices. Appendix \ref{appendix:training_workflow} describes step by step how to build classifiers from the data set. Appendix \ref{appendix:classification_features}. Appendix \ref{appendix:ror} . Appendix \ref{appendix:resources}. Appendix \ref{appendix:grotoap_distribution}.
\section{Authorship}

The work enclosed in this thesis has been implemented as a part of author's work duties in the Interdisciplinary Center for Mathematical and Computation Modeling at the University of Warsaw, in a team managed by ≈Åukasz Bolikowski and technically lead by Dominika Tkaczyk. Although the system has been created by a team of people, the attention was always paid to keeping distinctive portions of work sharply separated in terms of the authorship. Therefore it is possible to indicate which parts of the system where authored by whom. As a general rule, components of the system that were not created by the author of this thesis are not enclosed in the thesis or are described very briefly, with the authorship named explicitly. Since CERMINE is a successor of a legacy system based on Hidden Markov Models, some parts of it were inherited and possibly remodeled. Below we follow components that were not created by the author of the thesis:
\begin{itemize}
\item Interface with PDFBox for character extraction,
\item Text segmentation and structure extraction,
\item Web interface,
\item Reference extraction.
\end{itemize}

On the contrary, the following components were entirely designed and implemented by the thesis' author:
\begin{itemize}
\item Research on the related work (section \ref{sec:related_work}),
\item Design and implementation of the reading order resolution (section \ref{sec:reading_order}, appendix \ref{appendix:ror}),
\item Implementation of the classification features (section \ref{sec:feature_selection}, appendix \ref{appendix:features}),
\item Design and implementation of the classification workflow using Support Vector Machines (chapter \ref{sec:classification_workflow}),
\item Design and implementation of the classifiers' training workflow (appendix \ref{appendix:training_workflow})
\item Preparation and characterization of the training data set, i.e. GROTOAP2 (chapter \ref{sec:dataset}, appendix \ref{app:grotoap2}),
\item Optimization of the classifiers (section \ref{sec:svm_optimization}),
\item Evaluation of the initial and metadata classifiers (chapter \ref{chapter:evaluation}).
\end{itemize}


\subsection{On the language of this thesis}
In this thesis the first-person pronoun in plural form (``we'') has been used on several occasions. This form was adopted to follow the guidelines of scientific writing in english language. This thesis was written entirely by its author. Similarly, the depicted components of CERMINE were designed, developed and tested entirely be the author of this thesis. 
