\chapter{Introduction}
Digital libraries gained a lot of momentum in the recent years. The amount of publications held there is enormous and constantly growing. Collecting them in digital repositories raised many problems, where many of them go beyond acquisition issues - they have to be to be organized and classified in order to improve various retrieval procedures. Moreover, with time the purpose of the digital libraries has been evolving from a place where publications were stored, over search engines to quickly filter out publications relevant to certain topic, to frameworks used for analysis of collaboration patterns, researchers' professional network analysis, calculation of productivity and efficiency factors such as H-index (\cite{Hirsch}) or G-index or looking for emerging trends and topics in science.
All this research depends on a reliable source of full article texts and corresponding metadata, being both an input for analysis algorithms.

Even though there was a huge progress in science as such, scholarly publishing relies on methods invented more than two decades ago, which is an immense time in digital world. The most common format in digital publishing is PDF (Portable File Format), which focuses on how the document is rendered in reader's browser making it highly portable across hardware and software platforms, but without putting to much focus on metadata, which are understood here literally as \textit{data about data}. Articles are very often published and distributed in this format resulting in losing metadata. Their recovery can be performed either manually by a human worker or by a computer program. 

A modern, fully functional framework for digital libraries, in order to be useful and to provide high quality services, has to have access not only to the full texts of the articles and books that it stores, but also to their metadata. The metadata, data describing data, include such information as: document authors, document title, document abstract, keywords, document references and others. The purpose of obtaining the metadata is twofold. Firstly, they are indispensable when implementing a search engine, being an invaluable tool for all researchers looking for articles needed in their research field. Secondly, a subset of metadata, i.e. article authors and references are needed for various ranking algorithms based on citation count, such H-index \cite{Hirsch}, Impact Factor and other bibliometric indicators.

\section{Problem statement}
Regrettably, usually a digital library has to deal with the resources without any metadata provided by the publisher. In this case, the number of articles makes it barely possible to process them manually. It might also happen that the metadata available are not trustworthy or are faulty or partially missing. In these cases a digital library needs a way to obtain them automatically or semi-automatically.

This thesis presents CERMINE - a system for automatic extraction of metadata from scholarly publications, and GROTOAP2 - Ground-truth publication dataset. At its core CERMINE uses supervised machine learning algorithms, where labeled training samples from GROTOAP2 are used to train the system to recognize and assign individual elements of an article into classes depending on their function in an article. In a multi-stage process CERMINE extracts metadata, full text and bilbiographic references from an input article. The system was developed jointly by the author and his team mates as a part of work duties in Interdisciplinary Center for Mathematical and Computational Modeling at the University of Warsaw. However, their work will not be described extensively in this document and the authorship of each piece of the system will be marked explicitly. The system has been recently presented at the Force2015 meeting (\cite{Force2015}). Furthermore, an article on CERMINE was presented in the 11th IAPR International Workshop on Document Analysis Systems (\cite{DominikaTkaczykPaweSzostekMateuszFedoryszakPiotrJanDendek2014}). GROTOAP2 article was published in D-Lib magazine, volume 20. (\cite{DominikaTkaczykPaweSzostek2014}).
Information on the access to the source code and other resources is included in the appendix \ref{app:resources}.

\section{Related work}
Extracting metadata from scholarly publications is a well-studied problem. In the past, the algorithms expected scanned documents as an input and were generally formulated as image recognition problem. Those were built in the period when scholarly communication was present mainly or uniquely in a printed form and each article, before becoming available in a digital way, had to be scanned. Such approaches were presented in \cite{Thoma2001} and \cite{Flynn2007}.

Nowadays, a digital library has to cope with born-digital documents, where the letter recognition stage is omitted and processing starts with building up words and lines of text based on single characters. In the past several approaches using machine learning methods were presented.

In \cite{Esposito2008} there is presented a method for processing PostScript and PDF files using various machine learning methods in order to extract metadata.

\cite{Marinai2009} extracts basic metadata from scientific papers by, first of all, using various rules to perform segmantations of characters extracted from text and then employing a neural network to assign classes to blocks of text. Results are corrected by comparing them with an external bibliographic database.

In the approach proposed by \cite{Chen.2010} PDF files are first transformed into HTML format, then metadata is extracted using Hidden Markov Model. The authors test their approach using a set of 458 articles for VLDB conferences.

Both \cite{HuiHan} and \cite{HuiHan2005} present an approach of employing mixed Hidden Markov Model/Support Vector Machines algorithm to perform metadata and references extraction. Unlike in our approach, they do not take sequency-oriented information into considaration.

Finally, the TeamBeam algorithm, described in \cite{Kern2012}, uses a Maximum Entropy classifier to include sequence-related information in the classification. This algorithm is applied only to the first page of an article, therefore focusing only on metadata.

