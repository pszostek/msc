@article{Hirsch,
author= {{J. E. Hirsch}},
title = {{An index to quantify an individual's scientific research output}},
journal = {Proceedings of National Academy of Science of USA},
year = {2005}}

@article{A.McCallumK.Nigam2000,
author = {{A. McCallum}, K. Nigam and J. Rennie},
journal = {Information Retrieval},
pages = {127--163},
title = {{Automating the construction of internet portals with machine learning.}},
year = {2000}
}
@Iarticle{HuiHan,
author={Hui Han and Giles, C.L. and Manavoglu, E. and Hongyuan Zha and Zhenyue Zhang and Fox, E.A.},
booktitle={Digital Libraries, 2003. Proceedings. 2003 Joint Conference on},
title={Automatic document metadata extraction using support vector machines},
year={2003},
month={May},
pages={37-48}
}
@INPROCEEDINGS{Seymore99learninghidden,
    author = {Kristie Seymore and Andrew Mccallum and Ronald Rosenfeld},
    title = {Learning Hidden Markov Model Structure for Information Extraction},
    booktitle = {In AAAI 99 Workshop on Machine Learning for Information Extraction},
    year = {1999},
    pages = {37--42}
}

@inproceedings{A.StatnikovD.HardinI.Guyon2009,
author = {A. Statnikov, D. Hardin, I. Guyon and {C. Aliferis}},
booktitle = {AMIA Annual Symposium Proceedings},
title = {{A Gentle Introduction to Support Vector Machines in Biomedicine}},
year = {2009}
}
@article{Antonacopoulos2009,
abstract = {There is a significant need for a realistic dataset on which to evaluate layout analysis methods and examine their performance in detail. This paper presents a new dataset (and the methodology used to create it) based on a wide range of contemporary documents. Strong emphasis is placed on comprehensive and detailed representation of both complex and simple layouts, and on colour originals. In-depth information is recorded both at the page and region level. Ground truth is efficiently created using a new semi-automated tool and stored in a new comprehensive XML representation, the PAGE format. The dataset can be browsed and searched via a Web-based front end to the underlying database and suitable subsets (relevant to specific evaluation goals) can be selected and downloaded.},
author = {Antonacopoulos, A. and Bridson, D. and Papadopoulos, C. and Pletschacher, S.},
doi = {10.1109/ICDAR.2009.271},
isbn = {978-1-4244-4500-4},
issn = {1520-5363},
journal = {2009 10th International Conference on Document Analysis and Recognition},
keywords = {Performance evaluation,datasets,ground truth format,layout analysis,pge segmentation,region classification},
title = {{A Realistic Dataset for Performance Evaluation of Document Layout Analysis}},
year = {2009}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin  between the training patterns and the decision  boundary is presented. The technique  is applicable to a wide variety of classifiaction  functions, including Perceptrons, polynomials,  and Radial Basis Functions. The effective  number of parameters is adjusted automatically  to match the complexity of the problem.  The solution is expressed as a linear combination  of supporting patterns. These are the  subset of training patterns that are closest to  the decision boundary. Bounds on the generalization  performance based on the leave-one-out  method and the VC-dimension are given. Experimental  results on optical character recognition  problems demonstrate the good generalization  obtained when compared with other  learning algorithms.  1 INTRODUCTION  Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
doi = {10.1.1.21.3818},
isbn = {089791497X},
issn = {0-89791-497-X},
pages = {144--152},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
howpublished = "\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818}",
year = {1992}
}
@article{C.Cortes1995,
abstract = {In this paper, the optimal margin algorithm is generalized$\backslash$nto non-separable problems by the introduction of slack$\backslash$nvariables in the statement of the optimization problem.},
author = {{C.Cortes}, V. Vapnik},
doi = {10.1007/BF00994018},
issn = {08856125},
journal = {Machine Learning},
pages = {273--297},
title = {{Support Vector Networks}},
volume = {20},
year = {1995}
}
@article{Chang2011,
abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
author = {Chang, Chih-Chung and Lin, Chih-Jen},
doi = {10.1145/1961189.1961199},
isbn = {2157-6904},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {SVC,SVM,SVR,libSVM,support vector classification,support vector machine,support vector regression},
pages = {27:1----27:27},
title = {{LIBSVM: A Library for Support Vector Machines}},
howpublished = "\url{http://dl.acm.org/citation.cfm?doid=1961189.1961199}",
volume = {2},
year = {2011}
}
@article{Chen.2010,
author = {Chen., B. G. Cui and X.},
journal = {Advanced Intelligent Computing Theories and Applications},
pages = {205--212},
title = {{An Improved Hidden Markov Model for Literature Metadata Extraction}},
volume = {6215},
year = {2010}
}
@inproceedings{Tkaczyk2012,
author = {{Dominika Tkaczyk} and Artur Czeczko and Krzysztof Rusek and Lukasz Bolikowski and Roman Bogacewicz},
booktitle = {Proceedings of the 12th ACM/IEEE-CS joint conference on Digital Libraries},
pages = {381--382},
title = {{GROTOAP: ground truth for open access publications}},
year = {2012}
}
@inproceedings{DominikaTkaczykPaweSzostek2014,
author = {{Dominika Tkaczyk}, Pawel Szostek and Lukasz Bolikowski},
howpublished = "\url{http://www.dlib.org/dlib/november14/11contents.html}",
title = {{GROTOAP2 - The methodology for creating a large ground truth dataset of scientific articles}},
year = {2014}
}
@inproceedings{DominikaTkaczykPaweSzostekMateuszFedoryszakPiotrJanDendek2014,
author = {{Dominika Tkaczyk, Paweł Szostek, Mateusz Fedoryszak, Piotr Jan Dendek}, Lukasz Bolikowski},
booktitle = {11th IAPR International Workshop on Document Analysis Systems},
pages = {11--16},
title = {{CERMINE - automatic extraction of metadata and references from scientific literature}},
year = {2014}
}
@article{Esposito2008,
abstract = {In the last years, the spread of computers and the Internet caused a significant amount of documents to be available in digital format. Collecting them in digital repositories raised problems that go beyond simple acquisition issues, and cause the need to organize and classify them in order to improve the effectiveness and efficiency of the retrieval procedure. The success of such a process is tightly related to the ability of understanding the semantics of the document components and content. Since the obvious solution of manually creating and maintaining an updated index is clearly infeasible, due to the huge amount of data under consideration, there is a strong interest in methods that can provide solutions for automatically acquiring such a knowledge. This work presents a framework that intensively exploits intelligent techniques to support different tasks of automatic document processing from acquisition to indexing, from categorization to storing and retrieval. The prototypical version of the system DOMINUS is presented, whose main char- acteristic is the use of a Machine Learning Server, a suite of different inductive learning methods and systems, among which the more suitable for each specific doc- ument processing phase is chosen and applied. The core system is the incremental first-order logic learner INTHELEX. Thanks to incrementality, it can continuously update and refine the learned theories, dynamically extending its knowledge to han- dle even completely new classes of documents. Since DOMINUS is general and flexible, it can be embedded as a document management engine into many different Digital Library systems. Experiments in a real-world domain scenario, scientific conference management, confirmed the good performance of the proposed prototype.},
author = {Esposito, Floriana and Ferilli, Stefano and Basile, Teresa M A},
doi = {10.1007/978-3-540-76280-5\_5},
isbn = {978-3-540-76279-9, 978-3-540-76280-5},
issn = {1860-949X},
journal = {World Wide Web Internet And Web Information Systems},
pages = {1--35},
title = {{Machine Learning for Digital Document Processing : From Layout Analysis To Metadata Extraction}},
howpublished = "\url{http://www.springerlink.com/index/g06666kng13615pl.pdf}",
volume = {138},
year = {2008}
}
@inproceedings{Giles1998,
abstract = {We present CiteSeer: an autonomous citation indexing system which indexes academic literature in electronic format (e.g. Postscript files on theWeb). CiteSeer understands how to parse citations, identify citations to the same paper in different formats, and identify the context of citations in the body of articles. CiteSeer provides most of the advantages of traditional (manually constructed) citation indexes (e.g. the ISI citation indexes), including: literature retrieval by following citation links (e.g. by providing a list of papers that cite a given paper), the evaluation and ranking of papers, authors, journals, etc. based on the number of citations, and the identification of research trends. CiteSeer has many advantages over traditional citation indexes, including the ability to create more up-to-date databases which are not limited to a preselected set of journals or restricted by journal publication delays, completely autonomous operation with a corresponding reduction in cost, and powerful interactive browsing of the literature using the context of citations. Given a particular paper of interest, CiteSeer can display the context of how the paper is cited in subsequent publications. This context may contain a brief summary of the paper, another author’s response to the paper, or subsequentwork which builds upon the original article. CiteSeer allows the location of papers by keyword search or by citation links. Papers related to a given paper can be located using common citation information or word vector similarity. CiteSeer will soon be available for public use.},
author = {Giles, C Lee and Bollacker, Kurt D and Lawrence, Steve},
booktitle = {ACM Conference on Digital Libraries},
doi = {10.1145/276675.276685},
isbn = {0897919653},
keywords = {bibliometrics,citation context,citation indexing,literature search},
pages = {89--98},
title = {{CiteSeer: An Automatic Citation Indexing System}},
year = {1998}
}
@article{Giuffrida2000,
abstract = {The automatic document metadata extraction process is animportant task in a world where thousands of documents are just one``click'' away. Thus, powerful indices are necessary to support effective retrieval. The upcoming XML standard represents an important step in this direction as itssemistructuredrepresentation conveys document metadata together with the text of the document. For example, retrieval of scientific papers by authors or affiliations would be a straightforward tasks if papers were stored in XML.Unfortunately, today, the largest majority of documents on the web are available in forms that do not carryadditional semantics. Converting existing documents to a semistructured representation is time consuming and no automatic process can be easily applied. In this paper we discuss a system, based on a novel spatial/visualknowledge principle, for extracting metadata from scientific papers storedas PostScript files. Our system embeds the general knowledge about the graphical layout of a scientific paper to guide the metadata extraction process. Our system can effectively assist the automatic index creation for digital libraries.},
author = {Giuffrida, Giovanni and Shek, Eddie C. and Yang, Jihoon},
doi = {10.1145/336597.336639},
isbn = {158113231X},
journal = {International Conference on Digital Libraries},
pages = {77},
title = {{Knowledge-based metadata extraction from PostScript files}},
howpublished = "\url{http://portal.acm.org/citation.cfm?id=336597.336639}",
year = {2000}
}
@article{HaLee2003,
abstract = {Tools for visualizing and creating groundtruth and metadata are crucial for document image analysis research. In this paper we describe TrueViz (TRUEVIZ User's Manual, August 2000; Proceedings of the SPIE Conference on Document Recognition and Retrieval, San Jose, CA, 2001, pp. 1-12), which is a tool for visualizing and editing groundtruth/metadata. We first describe the groundtruthing task and the requirements for any interactive groundtruthing tool. Next we describe the system design of TrueViz and discuss how a user can use it to create groundtruth. TrueViz is implemented in the Java programming language and works on various platforms including Windows and Unix. TrueViz reads and stores groundtruth/metadata in XML format, and reads a corresponding image stored in TIFF image file format. Multilingual text editing, display, and search modules based on the Unicode representation for text are also provided. This software is being made available free of charge to researchers. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {{Ha Lee}, Chang and Kanungo, Tapas},
doi = {10.1016/S0031-3203(02)00101-2},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Annotation,Groundtruth,Java,Multilingual,Multiplatform,OCR,Visualization,XML},
pages = {811--825},
title = {{The architecture of TrueViz: A groundTRUth/metadata editing and VIsualiZing ToolKit}},
volume = {36},
year = {2003}
}
@article{Heroux2007,
abstract = {Performance evaluation for document image analysis and understanding is a recurring problem. Many ground- truthed document image databases are now used to evaluate algorithms, but these databases are less useful for the design of a complete system in a precise context. This paper proposes an approach for the automatic generation of synthesised document images and associated ground-truth information based on a derivation of publishing tools. An implementation of this approach illustrates the richness of the produced information.},
author = {Heroux, P. and Barbu, E. and Adam, S. and Trupin, E.},
doi = {10.1109/ICDAR.2007.4378755},
isbn = {978-0-7695-2822-9},
issn = {1520-5363},
journal = {Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
title = {{Automatic Ground-truth Generation for Document Image Analysis and Understanding}},
volume = {1},
year = {2007}
}
@article{Hirsch2005,
abstract = {I propose the index h, defined as the number of papers with citation number > or =h, as a useful index to characterize the scientific output of a researcher.},
archivePrefix = {arXiv},
arxivId = {physics/0509048},
author = {Hirsch, J E},
doi = {10.1073/pnas.0507655102},
eprint = {0509048},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {16569--16572},
pmid = {16275915},
primaryClass = {physics},
title = {{An index to quantify an individual's scientific research output.}},
volume = {102},
year = {2005}
}
@article{Marinai2009,
abstract = {In this paper we analyze our recent research on the use of document analysis techniques for metadata extraction from PDF papers. We describe a package that is designed to extract basic metadata from these documents. The package is used in combination with a digital library software suite to easily build personal digital libraries. The proposed software is based on a suitable combination of several techniques that include PDF parsing, low level document image processing, and layout analysis. In addition, we use the information gathered from a widely known citation database (DBLP) to assist the tool in the difficult task of author identification. The system is tested on some paper collections selected from recent conference proceedings.},
author = {Marinai, S.},
doi = {10.1109/ICDAR.2009.232},
isbn = {978-1-4244-4500-4},
issn = {1520-5363},
journal = {2009 10th International Conference on Document Analysis and Recognition},
keywords = {Digital Library,Layout Anlysis,Neural Network,PDF,XML},
title = {{Metadata Extraction from PDF Papers for Digital Library Ingest}},
year = {2009}
}
@article{Agrawal2009,
author={Agrawal, M. and Doermann, D.},
booktitle={Document Analysis and Recognition, 2009. ICDAR '09. 10th International Conference on},
title={Voronoi++: A Dynamic Page Segmentation Approach Based on Voronoi and Docstrum Features},
year={2009},
month={July},
pages={1011-1015},
keywords={computational geometry;document handling;image segmentation;Docstrum features;Voronoi++;document page segmentation;handwritten documents;neighborhood features;page content;printed documents;zone-based evaluation;Algorithm design and analysis;Carbon capture and storage;Communications technology;Educational institutions;Image analysis;Image segmentation;Magnetic separation;Performance evaluation;Text analysis;White spaces;adaptive;docstrum;dynamic;page segmentation;voronoi},
doi={10.1109/ICDAR.2009.270},
ISSN={1520-5363}
}

@article{O'Gorman1993,
author = {O'Gorman, L.},
journal = {IEEE Transactions onPattern Analysis and Machine Intelligence},
number = {11},
pages = {1162--1173},
title = {{The document spectrum for page layout analysis}},
volume = {15},
year = {1993}
}
@article{PaulFlynnLiZhouKurtMalyStevenZeil2007,
author = {{Paul Flynn, Li Zhou, Kurt Maly}, Steven Zeil and Mohammad Zubair},
journal = {ICADL},
pages = {327--336},
title = {{Automated Template-Based Metadata Extraction Architecture}},
volume = {4822},
year = {2007}
}
@article{Rigamonti2005,
abstract = { This article presents Xed, a reverse engineering tool for PDF documents, which extracts the original document layout structure. Xed mixes electronic extraction methods with state-of-the-art document analysis techniques and outputs the layout structure in a hierarchical canonical form, i.e. which is universal and independent of the document type. This article first reviews the major traps and tricks of the PDF format. It then introduces the architecture of Xed along with its main modules, and, in particular, the document physical structure extraction algorithm. Later on, a canonical format is proposed and discussed with an example. Finally the results of a practical evaluation are presented, followed by an outline of future works on the logical structure extraction.},
author = {Rigamonti, M. and Bloechle, J.L. and Hadjar, K. and Lalanne, D. and Ingold, R.},
doi = {10.1109/ICDAR.2005.242},
isbn = {0-7695-2420-6},
issn = {1520-5263},
journal = {Eighth International Conference on Document Analysis and Recognition (ICDAR'05)},
title = {{Towards a canonical and structured representation of PDF documents through reverse engineering}},
year = {2005}
}
@article{Sauvola1997,
abstract = {We describe a new approach to manage the testing of document
analysis and understanding applications. We propose and present a
collection of document images, a set of techniques to prepare the test
cases interactively and means to control the testing process. The
systems architecture is designed to be distributed, scalable and
platform independent utilizing Java, C++ and object-oriented databases.
The main features of this system are a basic document categorization and
ground truth, degradation models, custom test case creation facilities,
a test management module (pipelining, test history), the ability to
embed document analysis algorithms into the system, remote usage
facilities and robust graphical user interfaces},
author = {Sauvola, J. and Haapakoski, S. and Kauniskangas, H. and Seppanen, T. and Pietiklainen, M. and Doermann, D.},
doi = {10.1109/ICDAR.1997.620658},
isbn = {0-8186-7898-4},
journal = {Proceedings of the Fourth International Conference on Document Analysis and Recognition},
title = {{A distributed management system for testing document image analysis
algorithms}},
volume = {2},
year = {1997}
}
@inproceedings{Vapnik1999,
author = {Vapnik, Olivier Chapelle and Vladimir},
booktitle = {NIPS},
pages = {230--236},
title = {{Model selection for support vector machines}},
year = {1999}
}
@misc{CeON,
keywords = {CeON},
title = {GROTOAP2 at CeON},
howpublished = "\url{http://cermine.ceon.pl/}"
}

@misc{Marg,
keywords = {Marg},
title = {{MARG}},
howpublished = "\url{http://marg.nlm.nih.gov}"
}
@misc{Itext,
keywords = {itext},
title = {{iText}},
howpublished = "\url{http://itextpdf.com/}"
}
@misc{Uva,
keywords = {Uva},
title = {{UvA}},
howpublished = "\url{http://www.science.uva.nl/UvA-CDD/}"
}
@misc{Uw-iii,
keywords = {uw-iii},
title = {{UW-III}},
howpublished = "\url{http://www.science.uva.nl/research/dlia/datasets/uwash3.html}"
}
@misc{Yadda,
keywords = {yadda},
title = {{The virtual library of sicence}},
howpublished = "\url{http://http://yadda.icm.edu.pl/}"
}
@misc{Pubmed,
keywords = {Pubmed},
title = {{Pubmed}},
howpublished = "\url{ftp://ftp.ncbi.nlm.nih.gov/pub/pmc}"
}
@misc{PubmedXML,
keywords = {PubmedXML},
title = {{Pubmed XML description}},
howpublished = "\url{http://www.nlm.nih.gov/bsd/licensee/elements_descriptions.html}"
}
@misc{Force2015,
keywords = {Force2015},
title = {{Force2015: The future of Research Communication and e-Scholarship}},
howpublished = "\url{https://www.force11.org/meetings/force2015/demos-and-posters}",
year={2015}
}
@techreport{Chih-WeiHsu2010,
keywords = {Chih-WeiHsu2010},
author = {{Chih-Wei Hsu}, Chih-Chung Chang and Chih-Jen Lin},
title = {{A Practical Guide to Support Vector Classification}},
year = {2010}
}
@phdthesis{Choi,
author = {Choi, Jong Myong},
pages = {2010},
title = {{A Selective Sampling Method for Imbalanced Data Learning on Support Vector Machines}},
year = {2010}
}
