\chapter{Evaluation}\label{chapter:evaluation}
Evaluation of the final system was performed in an automatic way using GROTOAP2 documents as input. We used a 5-fold cross-validation to better simulate behavior of the classifiers on unknown data and to rule out ``lucky'' division into subsets. In this way we employed all the samples for both training and validation. In an $n$-fold cross-validation the data set is randomly partitioned into n subsets. In each of $n$ stages, a single subset is kept as the validation data and the remaining $n-1$ subsets are used as training data. This process is repeated $n$ times and the results are summed up together in a form of a confusion matrix. This matrix summarizes classification by saying what were the recognized and actual classes for the classified objects.
To quantify goodness of the classification process for every class we use precision and recall, which are two metrics complementing each other.

Precision of classification is the fraction of correctly classified elements from a class to all samples/zones classified as this class. In other words, precision says what is the chance of a sample belonging to some class if it was classified as this class. In the context of confusion matrices \ref{tab:initial_confusion_matrix} and \ref{tab:metadata_confusion_matrix}, precision is the fraction of the number typed in bold to the sum of the whole column.

Recall of classification is the fraction of correctly classified elements from a class to all samples from the class. It intuitively indicates what fraction of a class was classified properly. In the context of confusion matrices \ref{tab:initial_confusion_matrix} and \ref{tab:metadata_confusion_matrix}, recall is the fraction of the number typed in bold to the sum of the whole row.

\section{Initial classification evaluation}\label{sec:evaluation_initial}
For the evaluation of the initial classification we used 262'144 samples. This is much less than available in GROTOAP2, which includes 1'640'258 samples, since we needed to limit number of samples to keep the evaluation in a reasonable timescale. With the original number of samples the evaluation would not be possible, because:
\begin{itemize}
\item The training process involves matrix inversion, which has $\mathcal{O}(n^3)$ complexity with respect to the training set. This means that for the full documents set from GROTOAP2 it would take around 250 times more. With the limited number of samples the evaluation was done within 15 hours on a decent server machine.
\item \verb+libsvm+ has a single-thread implementation only. It is therefore impossible to fully use a multi-core machine with this tool. In our research we tried to use \verb+jlibsvm+, which is multi-threaded implementation of \verb+libsvm+ is Java, but it was not stable enough.
\end{itemize}
Table \ref{tab:initial_confusion_matrix} presents results of a 5-fold cross-validation, including a confusion matrix, as well as precision and recall values. We can state, that the first stage of classification yields good results. All the classes have precision over 95\%, which is an extraordinary number. Conversely, recall of the \verb+REFERENCE+ class is equal to 87\%, since a lot of references were classified as body elements. This issue needs to be addressed in next releases of CERMINE.

\begin{table*}[]
\centering
\begin{tabular}{|r|c|c|c|c||c|c|}
\hline
& \rotatebox{90}{\textbf{BODY}} & \rotatebox{90}{\textbf{METADATA}} & \rotatebox{90}{\textbf{REFERENCES }} & \rotatebox{90}{\textbf{OTHER}} & \rotatebox{90}{\textbf{precision}} & \rotatebox{90}{\textbf{recall}} \\ \hline \hline
\textbf{BODY} & \textbf{167467} & 1394 & 350 & 769 & 96.95 & 98.52 \\ \hline
\textbf{METADATA} & 2094 & \textbf{51707} & 52 & 392 & 95.99 & 95.32  \\ \hline
\textbf{REFERENCES} & 1457 & 45 & \textbf{9641} & 97 & 95.67 & 87.77 \\ \hline
\textbf{OTHER} & 1724 & 719 & 34 & \textbf{24202}& 95.06 & 90.72 \\ \hline
\end{tabular}
\caption{Confusion matrix for the initial zone classification in CERMINE obtained in a 5-fold cross-validation. Rows and columns contain desired and obtained labels respectively.}
\label{tab:initial_confusion_matrix}
\end{table*}
\section{Metadata classification evaluation}
For the evaluation of the metadata classification we used 341'368 samples, this is all the samples available in GROTOAP2. Evaluation of the metadata classification was done independently from the first stage, i.e. from initial classification. We therefore assumed that the first stage is done without errors (precision = recall = 100\%) and all the metadata are passed to this stage. Similarly to what was done in \ref{sec:evaluation_initial}, we performed a 5-fold cross-validation whose results are presented in the table \ref{tab:metadata_confusion_matrix}. All the classes were classified with precision greater than 90\%. The one with the lowest precision is \verb+correspondence+, since often \verb+bib_info+, \verb+author+ and \verb+affiliation+ were classified as this class.

Recall is in most cases held at a high level (over 92\%), with the exception of \verb+type+ and \verb+keywords+. The former is often misclassified as \verb+bib_info+, the latter is misclassified as \verb+bib_info+ and \verb+abstract+. These errors are very likely caused by imprecise segmentation algorithm, as well as errors in GROTOAP2 matching algorithm, what should be addressed in the next release in order to achieve better performance.

In order to evaluate complete CERMINE performance, we need to take into account the fact that not all the zones in the initial classification are classified correctly. Let us consider \verb+title+ as an example in this analysis. Only 95\% of all metadata zones are correctly classified and only 95\% of all zones classified as metadata are real metadata. \verb+title+'s recall and precision are equal to 98\%. We might estimate that 93\% of all zones classified as \verb+title+ are real titles, the remaining 7\% are random.
\vspace{2cm}
Good classification results have to be considered from two perspectives:
\begin{itemize}
\item the results are comparable to the state-of-the-art system presented in \cite{Kern2012}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\newgeometry{margin=1cm}
\thispagestyle{empty}
\begin{sidewaystable}[clockwise]
\centering
\begin{tabular}{|r||c|c|c|c|c|c|c|c|c|c|c|c||c|c|}
\hline \parbox[b]{\hsize}
& \rotatebox{65}{\textbf{abstract}} & \rotatebox{65}{\textbf{affiliation}} & \rotatebox{65}{\textbf{author}} & \rotatebox{65}{\textbf{bib\_info}} & \rotatebox{65}{\textbf{copyright}} & \rotatebox{65}{\textbf{correspondence}} & \rotatebox{65}{\textbf{dates}} & \rotatebox{65}{\textbf{editor}} & \rotatebox{65}{\textbf{keywords}} & \rotatebox{65}{\textbf{title}} & \rotatebox{65}{\textbf{title\_author}} & \rotatebox{65}{\textbf{type}} & \rotatebox{65}{\textbf{precision[\%]}} & \rotatebox{65}{\textbf{recall[\%]}} \\
\hline \hline
\textbf{abstract} & \textbf{30583} & 27 & 8 & 403 & 33 & 42  & 5  & 2 & 85 & 7 & 0 & 25 & 98.18 & 97.96 \\ \hline
\textbf{affiliation} & 40 & \textbf{17483} & 39 & 257 & 19 & 365 & 26 & 19 & 22 & 3 & 1 & 11 & 95.55 & 97.96 \\ \hline
\textbf{author}  & 12 & 59 & \textbf{12116} & 270 & 5 & 264 & 2 & 3 & 3 & 6 & 5 & 14 & 97.71 & 94.46 \\ \hline
\textbf{bib\_info} & 278 & 117 & 98 & \textbf{208099} & 117 & 496 & 317 & 2 & 133 & 66 & 10 & 359 & 97.62 & 99.05\\ \hline
\textbf{copyright} & 40 & 13 & 13 & 956 & \textbf{14419} & 58 & 40 & 0 & 5 & 1 & 14 & 25 & 95.72 & 92.58 \\ \hline
\textbf{correspondence} & 48 & 535 & 62 & 239 & 12 & \textbf{7783} & 15 & 1 & 12 & 1 & 0 & 0 & 90.04 & 89.38\\ \hline
\textbf{dates} & 13 & 5 & 2 & 1115 & 35 & 6 & \textbf{14026} & 1 & 11 & 4 & 1 & 0 & 97.15 & 92.16 \\ \hline
\textbf{editor} & 1 & 34 & 4 & 1 & 0 & 0 & 1 & \textbf{2264} & 0 & 0 & 0 & 0 & 98.69 & 98.22 \\ \hline
\textbf{keywords} & 104 & 23 & 21 & 880 & 1 & 9 & 3 & 2 & \textbf{4546} & 21 & 0 & 23 & 93.67 & 80.70 \\ \hline
\textbf{title} & 17 & 1 & 11 & 66 & 4 & 0 & 1 & 0 & 3 & \textbf{11642} & 25 & 21 & 98.61 & 98.74 \\ \hline
\textbf{title\_author} & 3 & 0 & 1 & 22 & 6 & 0 & 0 & 0 & 0 & 33 & \textbf{1565} & 0 & 96.53 & 96.00 \\ \hline
\textbf{type} & 12 & 0 & 25 & 872 & 33 & 0 & 2 & 0 & 33 & 22 & 0 & \textbf{7148} & 93.73 & 87.74 \\ \hline
\bottomrule
\end{tabular}
\caption{Confusion matrix obtained for classification of the metadata in CERMINE in a 5-fold cross-validation using documents from GROTOAP2. Rows and columns contain desired and obtained labels respectively.}
\label{tab:metadata_confusion_matrix}
\end{sidewaystable}

\clearpage
\restoregeometry
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future work}
\subsection{GROTOAP2}
GROTOAP2 is a rich data set created in a semi-automatized way from Open Access publications. It contains 13210 articles representing a big variety of layouts and issued by more than 200 publishers. We find this set very useful for every data mining algorithm willing to extract metadata, data and other various features from scholarly articles in an automated way. GROTOAP2 can be easily applied to testing or learning every algorithm designed for processing born-digital documents.
\quad
We believe that GROTOAP2 can be made more versatile by:
\begin{itemize}
\item Growing its size by an order or two orders of magnitude.
\item Improving the algorithm used to assign NML metadata to text zones by tuning the magic constants used there.
\item Tuning the segmentation algorithm and improving segmentation quality. This would result in removing the \verb+author_title+ tag.
\item Dividing the \verb+body_content+ into subcategories based on the level of their headers.
\item Including fine-grained author metadata, e.g. first name, middle name, surname.
\item Introducing association between authors and theirs affiliations, i.e. pointing out which author is associated with which affiliation.
\item Including fine-grained bibliographic reference data, making it applicable for reference-extraction algorithms.
\end{itemize}
Most of the above-mentioned cases are related to loosing as little information as possible from the original Pubmed XML files.

\subsection{CERMINE}
CERMINE is a robust system for metadata extraction. It can be used to accurately extract fine-grained metadata from scientific articles en masse. CERMINE can be very useful in every digital library application, as it is flexible and does not depend on any particular layout. What is more, it can be easily adapted in a semi-automatic way to some particular layout, if such need arises.


We consider that there are many ways to improve the system, e.g.:
\begin{itemize}
\item increase classification accuracy and recall. This can be done by:
	\begin{itemize}
		\item choosing a different classification algorithm,
		\item using an ensemble of algorithms,
		\item performing lexical analysis on the zones classified as unknown,
		\item increasing quality of the training set.
	\end{itemize}
\item extract detailed bibliographic references,
\item extract figures and tables,
\item perform sentiment analysis with respect to the included references,
\item improve segmentation algorithm's on unusual layouts.
\end{itemize}